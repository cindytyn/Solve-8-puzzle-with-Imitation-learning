# -*- coding: utf-8 -*-
"""417-model-NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12SfI2dlbLj3Iwm9r7WLAMmuAVbJttqfs
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/Courses/CMPT 417/Final Project/s3_train3_csv.csv")
df = df.iloc[:,1:]
df.head()

df.info()

df_test = pd.read_csv("/content/drive/MyDrive/Courses/CMPT 417/Final Project/s3_test2_csv.csv")
df_test = df_test.iloc[:,1:]
df_test.head()

df_test.info()

pList = []
cList = []
pList.append(df['previous_state'])
cList.append(df['current_state'])

pList_test = []
cList_test = []
pList_test.append(df_test['previous_state'])
cList_test.append(df_test['current_state'])

def flatten_list(configList):
  flatten_list = []
  for x in configList:
    for y in x: #this gets [[0, 1, 3], [7, 8, 2], [5, 4, 6]] [[0, 1, 3], [7, 8, 2], [5, 4, 6]]
      nested_list = []
      for s in y: #this gets every element as a string
        if s.isdigit():
          nested_list.append(int(s))
      flatten_list.append(nested_list)
  print(flatten_list)
  return flatten_list

X = flatten_list(pList)
Y = flatten_list(cList)

X_test = flatten_list(pList_test)
Y_test = flatten_list(cList_test)

x = np.array(X)
x_test = np.array(X_test)

x = pd.DataFrame(X)
x_test = pd.DataFrame(X_test)

x.head()

y = np.array(Y)
y_test = np.array(Y_test)

import tensorflow as tf
from tensorflow.keras import layers

x_array = X_train.values
x_tensor = tf.convert_to_tensor(x_array)

Val_array = Val_train.values
Val_tensor = tf.convert_to_tensor(Val_array)

y_tensor = tf.convert_to_tensor(y_train)
Val_y_tensor = tf.convert_to_tensor(Val_y_train)

x_tensor = tf.reshape(x_tensor, (X_train.shape[0], X_train.shape[1], 1))  
Val_tensor = tf.reshape(Val_tensor, (Val_train.shape[0], Val_train.shape[1], 1))

y_tensor = tf.reshape(y_tensor, (y_train.shape[0],1 , y_train.shape[1]))  
Val_y_tensor = tf.reshape(Val_y_tensor, (Val_y_train.shape[0], 1, Val_y_train.shape[1]))

x_tensor[0].shape

y_tensor[0].shape

epochs = 20
model = tf.keras.Sequential()
model.add(layers.Conv1D(32, 2, activation='relu', input_shape=x_tensor[0].shape))
model.add(layers.BatchNormalization())
model.add(layers.MaxPooling1D(pool_size=(2), strides=(2)))

model.add(layers.Conv1D(64, 2, activation='relu'))
model.add(layers.Conv1D(64, 2, activation='relu'))

model.add(layers.BatchNormalization())
model.add(layers.MaxPooling1D(pool_size=(2), strides=(2)))


model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))

model.add(layers.Dense(32, activation='relu'))


model.add(layers.Dense(9, activation='softmax'))

model.add(layers.Reshape(( 1, 9)))


model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.summary()

"""> sparse_categorical_crossentropy is defined as categorical crossentropy with integer targets"""

# train the model
history = model.fit(x_tensor, y_tensor, epochs=epochs, validation_data=(Val_tensor, Val_y_tensor), verbose=1)

tf.keras.utils.plot_model(model, '{}.png'.format(model.name), show_shapes=True, dpi =50)

# Plot the loss curves for training and validation
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, epochs + 1)

plt.figure(figsize=(8,6))
plt.plot(epochs, loss, 'bo', label='Training loss', color='red')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.grid()
plt.legend()
plt.show()

# Plot the accuracy curves for training and validation
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
epochs = range(1, 20 + 1)

plt.figure(figsize=(8,6))
plt.plot(epochs, acc, 'bo', label='Training accuracy', color='red')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.grid()
plt.legend()
plt.show()

test_array = x_test.values
test_tensor = tf.convert_to_tensor(test_array)

test_y_tensor = tf.convert_to_tensor(y_test)

test_tensor = tf.reshape(test_tensor, (x_test.shape[0], x_test.shape[1], 1))  
test_y_tensor = tf.reshape(test_y_tensor, (y_test.shape[0], 1, y_test.shape[1]))

# Compute loss and accuracy for our testing dataset
evaluations = model.evaluate(test_tensor, test_y_tensor, verbose=1, return_dict=True)

print('Testing loss: {0:.6f}'.format(evaluations['loss']))
print('Testing accuracy: {0:.6f}'.format(evaluations['accuracy']))

"""LSTM"""

from sklearn.model_selection import train_test_split

X_train, Val_train, y_train, Val_y_train = train_test_split(x, y, test_size=0.25, shuffle=False)

x_array = X_train.values
x_tensor = tf.convert_to_tensor(x_array)

Val_array = Val_train.values
Val_tensor = tf.convert_to_tensor(Val_array)

y_tensor = tf.convert_to_tensor(y_train)
Val_y_tensor = tf.convert_to_tensor(Val_y_train)

x_tensor = tf.reshape(x_tensor, (X_train.shape[0], X_train.shape[1], 1))  
Val_tensor = tf.reshape(Val_tensor, (Val_train.shape[0], Val_train.shape[1], 1))

y_tensor = tf.reshape(y_tensor, (y_train.shape[0],1 , y_train.shape[1]))  
Val_y_tensor = tf.reshape(Val_y_tensor, (Val_y_train.shape[0], 1, Val_y_train.shape[1]))

from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout, BatchNormalization, MaxPooling1D

epochs = 20

# Define the LSTM model
model = Sequential()
model.add(LSTM(128, input_shape=x_tensor[0].shape, return_sequences=True))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(LSTM(128))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(layers.Dense(9, activation='softmax'))
model.add(layers.Reshape(( 1, 9)))

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

model.summary()

# train the model
history = model.fit(x_tensor, y_tensor, epochs=epochs, validation_data=(Val_tensor, Val_y_tensor), verbose=1)

tf.keras.utils.plot_model(model, '{}.png'.format(model.name), show_shapes=True, dpi =50)

# Plot the loss curves for training and validation
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, 20 + 1)

plt.figure(figsize=(8,6))
plt.plot(epochs, loss, 'bo', label='Training loss', color='red')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('LSTM Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.grid()
plt.legend()
plt.show()

# Plot the accuracy curves for training and validation
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
epochs = range(1, 20 + 1)

plt.figure(figsize=(8,6))
plt.plot(epochs, acc, 'bo', label='Training accuracy', color='red')
plt.plot(epochs, val_acc, 'b', label='Validation accuracy')
plt.title('LSTM Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.grid()
plt.legend()
plt.show()

test_array = x_test.values
test_tensor = tf.convert_to_tensor(test_array)

test_y_tensor = tf.convert_to_tensor(y_test)

test_tensor = tf.reshape(test_tensor, (x_test.shape[0], x_test.shape[1], 1))  
test_y_tensor = tf.reshape(test_y_tensor, (y_test.shape[0], 1, y_test.shape[1]))

# Compute loss and accuracy for our testing dataset
evaluations = model.evaluate(test_tensor, test_y_tensor, verbose=1, return_dict=True)

print('Testing loss: {0:.6f}'.format(evaluations['loss']))
print('Testing accuracy: {0:.6f}'.format(evaluations['accuracy']))